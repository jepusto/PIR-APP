---
title: "Wood et al Data Memo"
author: "Daniel M. Swan and James E. Pustejovsky"
date: "June 2, 2015"
output: pdf_document
---

```{r setup, echo=FALSE, fig.width = 7, fig.height = 7, message = FALSE}
library(ARPobservation)
library(ggplot2)
library(plyr)
library(knitr)
library(Pusto)
#source("R/PIR-APP.R")
#wood <- read.csv("data/Wood et al data.csv", na.strings = ".", stringsAsFactors = FALSE)
source("../R/PIR-APP.R")
wood <- read.csv("Wood et al data.csv", na.strings = ".", stringsAsFactors = FALSE)
```

```{r bootstrap, echo = FALSE, cache = TRUE, warning = FALSE, message = FALSE}

PIR_wrap <- function(data_line, c = 1, d = 0, k_prior = 1, theta_prior = Inf, iterations = 5000) {
  
  U <- data_line[4:70]
  U <- as.numeric(U[(U == 1|U == 0) & !is.na(U)])
  
  results <- PIRbootstrap(U = U, c = c, d = d, coding = data_line[3], 
         penalty_func = Beta_Gamma(k_mu = k_prior, k_lambda = k_prior, 
                                   theta_mu = theta_prior, 
                                   theta_lambda = theta_prior), transform = "exp", iterations = iterations)
  
  summary <- mean(U)
  
  results$summary <- summary
  results
}

source_func <- ls()

invisible(cluster <- start_parallel(source_func, message = FALSE))
invisible(clusterEvalQ(cluster, library(ARPobservation)))

system.time(ple <- adply(wood, 1, PIR_wrap, k_prior = 1.5, theta_prior = 10, .parallel = TRUE, iterations = 2000))

stopCluster(cluster)

ple <- ple[,-(4:70)]

cdr <- read.csv("transcribed.csv")[,-1]

ple <- merge(ple, cdr)

ple <- within(ple,{
  CDR <- CDR/100
  MTS <- MTS/100
})

PIR <- subset(ple, Measurement.Type == "PIR")
WIR <- subset(ple, Measurement.Type == "WIR")

names(PIR)[5:9] <- paste0("PIR", names(PIR[5:9]))
names(WIR)[5:9] <- paste0("WIR", names(WIR[5:9]))

ple_wide <- merge(PIR[,-3],WIR[,-3])

save(ple, ple_wide, file = "Wood PLE.RData")
```

In this memo, we describe an application of penalized likelihood estimators (PLEs) for partial interval recording and whole interval recording to a subset of the data from Wood, Hojnoski, Laracy, and Olson (2015). These data contain direct observations of childrens' academic engagement behavior, recorded using several different methods including both PIR and WIR, as well as momentary time sampling and continuous duration recording. 

Academic engagement is a _state behavior_ (sometimes called a _duration-based behavior_), in that each bout of the behavior has a positive duration. State behavior has two primary characteristics: _prevalence_ (also called _percentage duration_), which is the true proportion of time the behavior occurs, and _incidence_ (also called _rate_), which is the frequency with which new bouts of behavior begin, per unit time. 

Continuous duration recording (CDR) is theoretically the ideal method for direct observation of behavior because it provides simple, unbiased estimates of _both_ prevalence and incidence. However, because it requires the observer to maintain sustained attention for the entire observation session, CDR is not always used in field settings. More frequently, some form of interval recording procedure is used. Partial interval recording (PIR), whole interval recording (WIR), and momentary time sampling (MTS) are three common, well-known methods used in direct observation of behavior. PIR slices up each observation session into a number of equal-length intervals; for instance a 15 minute observation session might be cut into 45 20-second intervals. Any interval containing the behavior, no matter how brief the behavior, is scored as 1 and any interval with no instance of the behavior is scored as a 0. WIR is similar, except that an interval must contain the behavior for its whole length to be scored a 1 and is otherwise scored a 0. In contrast, MTS records the presence of the behavior at the "moment"" at beginning or end of each interval, giving each moment a score of 1 if the behavior is present and a score of 0 if it is absent.  

Usually, the interval scores from a session are summarized as the proportion of intervals with behavior. In the case of PIR and WIR, this summary measure is usually interpretted as an estimate of prevalence. However,the summary proportion is actually affected by both prevalence and incidence, with PIR systematically overestimating and WIR systematically underestimating prevalence. In the case of MTS, the summary proportion is an unbiased estimate of prevalence under very weak assumptions, but we are still left without any estimate of incidence. 

In addition to the fact that PIR and WIR systematically mis-estimate prevalence, a further problem with all three methods is that they only estimate prevalence, when incidence may be of interest as well. Consider that a behavior with high prevalence and low incidence is very different from one with high prevalence and high incidence. In the case of high prevalence and low incidence, you might have a child who is actively engaged in learning for a large proportion of the time with only a very small number of instances where they are off task. In the case of high prevalence and high incidence, you have a child who is "engaged" for short bouts, but also has many instances of off-task behavior. These very different scenarios likely lead to very different learning outcomes, yet with only an estimate of prevalence we might characterize these two scenarios as being very similar. 

The PLE method that we are developing is an attempt to give researchers who use interval recording procedures a method of estimating both prevalence and incidence, while avoiding the systematic biases of the PIR and WIR summary scores. The methods build on earlier work by Brown, Solomon, and Stephens (1977) that developed maximum likelihood estimators for MTS data. 

# Data and Analysis

The original data for Wood et al (2015) came from 13 video-taped sessions of 24 target student participants, with between one and four children captured by each video. The first author coded the sessions using PIR, WIR, and MTS. A senior graduate student coded the videos using continuous duration recording, so that the first authors' coding would not be influenced by knowledge of the "true" value of prevalence. The authors employed a variant of PIR sometimes seen in the direct observation literature, which we refer to as fractional interval recording (FIR). In FIR, rather than marking any interval containing the target behavior a 1, the behavior must last some pre-determined proportion of the interval to be considered present in the interval. In the present study, the behavior needed to last for at least 5 seconds (1/3 of the interval) to be considered present in the interval. The FIR data from the paper does not precisely conform to the modeling assumptions behind our PLE method. This discrepancy therefore allows us to examine the performance of the PLEs when the model is mis-specified. The WIR estimates were recorded using the conventional rule, and so the WIR data llows us to examine performance when this aspect of the model is correct.

We were provided with scans of the original hard copies of interval level-data for 16 of the 24 participants in the study. Two coders transcribed the data to a spreadsheet independently, and then the data was checked for agreement. For the purposes of this analysis, any missing intervals in the middle of the observation session were discarded and we treated the intervals containing data as the "complete" record, without accounting for missingness in our model.

In order to benchmark the PLEs, we hand-transcribed the continuous duration recording values of prevalence from Table 1 of Wood et al (2015), along with the MTS values, teacher ratings, and expert ratings, by matching the summary values of both PIR and WIR to the other estimates in the table. Both the CDR and MTS values are displayed in this document as proportions rather than percentages to conform to the convention we typically use. Confidence intervals for the PLEs were obtained via a parametric bootstrap procedure.

# Prevalence and Incidence 

```{r prep,echo=FALSE}
# load("data/Wood PLE.RData")
phi <- ple[ple$parm == "phi",]
names(phi)[4:8] <- paste0("phi", names(phi)[4:8])
zeta <- ple[ple$parm == "zeta",]
names(zeta)[4:8] <- paste0("zeta", names(zeta)[4:8])

parms_wide <- merge(phi[,-4], zeta[,-4])
```

```{r PIRpz, echo = FALSE}
pz <- ggplot(parms_wide, aes(x = phiest,
                             y = zetaest,
                             color = Measurement.Type))

pz + geom_point()+
     labs(title = "Estimates of prevalence and incidence",
          x = "Prevalence",
          y = "Incidence")+
     theme_bw()

# qplot(phiest, zetaest, geom = "point", color = Measurement.Type,
#       main = "Estimates of prevalence and incidence",
#       xlab = "Prevalence",
#       ylab = "Incidence",
#       data = parms_wide) +
#       theme_bw()
```

The first plot displays both PIR and WIR estimates of prevalence plotted against incidence (per interval). It appears that the WIR estimates have higher incidence and higher prevalence than the PIR estimates. This is probably an artifact of the fact that FIR (modified PIR) was used to score the observations. However, without comparing these values to the true values we can't know if the difference is due to systematic bias, which of the observation methods is biased, and how we might characterize this bias.

# Prevalence

```{r PIR_WIR, echo=FALSE}
prev <- ggplot(subset(ple_wide, parm == "phi"), 
                aes(x = WIRest, y = PIRest))

prev + geom_point() +
#       stat_smooth(method = "lm", se = FALSE) +
        labs(title = "PIR and WIR estimates of prevalence",
        x= "WIR", y = "PIR") +
        coord_cartesian(xlim = c(0,1), ylim = c(0,1)) +
        theme_bw() +
        geom_abline(slope = 1, intercept = 0, color = "grey") +
        theme(legend.position="none")

PIRcovered <- with(PIR[PIR$parm == "phi",],mean(PIRCI_L <= CDR & PIRCI_U >= CDR))
WIRcovered <- with(WIR[WIR$parm == "phi",],mean(WIRCI_L <= CDR & WIRCI_U >= CDR))
```

The second plot displays the PLE estimates of prevalence from PIR and WIR plotted against one another. The grey line is a line that passes through $x = y$, where the points would lay if the agreement between the PIR and WIR observations was exact. The plot suggests that the PIR estimates of prevalence are generally lower than the WIR estimates. It is possible that this is attributable to the fact that the PIR used in Wood et al (2015) was a modified version (FIR). Putting a minimum on the length of time required for a behavior to "count" as having occurred likely reduced the overall upward bias on prevalence, bias that we attempt to account for in our model.

```{r PIRWIRCDR, echo = FALSE, fig.height = 8}
qplot(CDR, est, ymin = CI_L, ymax = CI_U, color = Measurement.Type,
      position = position_jitter(w = .05),
      geom = "pointrange",
      main = "Estimates plotted against CDR proportion",
      xlab = "CDR",
      ylab = "Estimates",
      data = subset(ple, parm == "phi"))+
      geom_abline(slope = 1, intercept = 0, color = "grey")+
      coord_cartesian(xlim = c(0.5,1), ylim = c(0,1))+
      theme_bw()+
      theme(legend.position = "none")+
      facet_wrap(~Measurement.Type, nrow = 2)
```

The third plot displays the PLE estimates of prevalence from both PIR and WIR plotted against the CDR proportion, which we treat as a benchmark for the PLEs. The vertical bars overlaid on the points represents the parametric bootstrap confidence intervals. The grey line is a line through $x = y$, where all of the points would lie if there was perfect agreement between CDR and the other observation methods. Confidence intervals that intersect with the grey line indicate that they cover the "true" score. The PIR estimates appear to under-estimate of prevalence, while the WIR estimates over-estimate, but to a much lesser degree. The WIR CIs mostly cover the CDR proportion (coverage = `r round(WIRcovered, digits =2)`), while the PIR CIs coverage is slightly lower (coverage = `r round(PIRcovered, digits = 2)`).

```{r meas_error, echo = FALSE}
phi$error <- with(phi, (CDR - phiest)/CDR)
qplot(Participant, error, fill = Measurement.Type, stat = "identity", 
      position = "dodge", geom = "bar", data = phi)+
  scale_x_continuous(breaks = 1:24)+
  theme_bw()+
  theme(legend.position = "top", legend.title = element_blank())
```

The fourth plot displays measurement error relative to the CDR value of prevalence. Unlike the original manuscript, this plot doesn't show the same consistent downward bias for WIR data, although the upward bias for PIR data is still present. The magnitude of the bias for PIR appears slightly larger than in the original manuscript; the magnitude of the bias for the WIR estimates is considerably lower.

```{r mse, echo = FALSE}
phi$sq_error <- with(phi, (phiest - CDR)^2)
phi$sq_errorMTS <- with(phi, (MTS - CDR)^2)
phi$sq_errorSum <- with(phi, (summary - CDR)^2)
mean_sq_error <- ddply(phi, .(Measurement.Type), summarize, 
                       MSE = round(mean(sq_error),4), 
                       MSEMTS = round(mean(sq_errorMTS),4), 
                       MSEsum = round(mean(sq_errorSum), 4))
mse <- mean_sq_error[c(1,2,4)]
mse <- rbind(mse, c("MTS", "-", mean_sq_error[1,3]))
kable(mse, col.names = c("Measurement Type", "PLE", "Summary"), caption = "Error", digits = 4)
```

Table 1 reports the _mean-squared error_ (MSE) of the PLEs as well as that of the summary measurements, for each of the three recording procedures, where the MSE is defined as 
$$MSE = \frac{1}{n}\sum_{i = 1}^{n} (\hat\theta_i - CDR_i)^2,$$ 
for a prevalence estimate $\hat\theta_i$ and CDR score $CDR_i$. The error for the PIR PLE is about twice that of the summary measurement, while the error for WIR PLE is very small compared to the summary measurement. The WIR error is comparable to the MTS error, which is excellent considering that MTS estimates are generally considered to be "unbiased" under very minimal assumptions. The discrepancey between PIR and WIR error is probably an issue of the model not accounting for the modified PIR method (FIR) used to collect these data, whereas our model is appropriately specified for the WIR data.

```{r prev_cor, echo = FALSE, fig.align = 'left', warning = FALSE}
library(reshape2)
Tcors <- ddply(ple[ple$parm == "phi",], .(Measurement.Type), summarize,
               Summary_cor = cor(summary, Teacher, method = "spearman"),
               MTS_cor = cor(MTS, Teacher, method = "spearman"),
               CDR_cor = cor(CDR, Teacher, method = "spearman"),
               Est_cor = cor(est, Teacher, method = "spearman"),
               Summary_p = cor.test(summary, Teacher, method = "spearman")$p.value,
               MTS_p = cor.test(MTS, Teacher, method = "spearman")$p.value,
               CDR_p = cor.test(CDR, Teacher, method = "spearman")$p.value,
               Est_p = cor.test(est, Teacher, method = "spearman")$p.value)

Tcors_melt <- melt(Tcors, measure.vars = 2:9, id.vars = 1)
Tcors_melt <- Tcors_melt[c(1:3,5,7:11,13,15:16),]
Tcors_melt$Measurement.Type <- c("PIR", "WIR", "MTS", "CDR", "PIR", "WIR")
Tcors_melt$variable <- c("Summary", "Summary", "Summary", "Summary", "PLE", "PLE",
                         "Summary_p", "Summary_p", "Summary_p", "Summary_p",
                         "PLE_p", "PLE_p")
Tcors_table <- dcast(Tcors_melt, Measurement.Type ~ variable)
Tcors_table[,2:5] <- round(Tcors_table[, 2:5], digits = 2)
Tcors_table[1:2,2:3] <- "-"
Tcors_table <- Tcors_table[c(3,4,2,1),]

Ecors <- ddply(ple[ple$parm == "phi",], .(Measurement.Type), summarize,
               Summary_cor = cor(summary, Expert.Rater, method = "spearman"),
               MTS_cor = cor(MTS, Expert.Rater, method = "spearman"),
               CDR_cor = cor(CDR, Expert.Rater, method = "spearman"),
               Est_cor = cor(est, Expert.Rater, method = "spearman"),
               Summary_p = cor.test(summary, Expert.Rater, method = "spearman")$p.value,
               MTS_p = cor.test(MTS, Expert.Rater, method = "spearman")$p.value,
               CDR_p = cor.test(CDR, Expert.Rater, method = "spearman")$p.value,
               Est_p = cor.test(est, Expert.Rater, method = "spearman")$p.value)

Ecors_melt <- melt(Ecors, measure.vars = 2:9, id.vars = 1)
Ecors_melt <- Ecors_melt[c(1:3,5,7:11,13,15:16),]
Ecors_melt$Measurement.Type <- c("PIR", "WIR", "MTS", "CDR", "PIR", "WIR")
Ecors_melt$variable <- c("Summary", "Summary", "Summary", "Summary", "PLE", "PLE",
                         "Summary_p", "Summary_p", "Summary_p", "Summary_p",
                         "PLE_p", "PLE_p")
Ecors_table <- dcast(Ecors_melt, Measurement.Type ~ variable)
Ecors_table[,2:5] <- round(Ecors_table[, 2:5], digits = 2)
Ecors_table[1:2,2:3] <- "-"
Ecors_table <- Ecors_table[c(3,4,2,1),]

kable(Tcors_table, 
        col.names = c("Observational Method", "PLE", "p-value", "Summary", "p-value"),
        row.names = FALSE,
      caption = "Spearman's Rho - Teacher Ratings")

kable(Ecors_table, 
       col.names = c("Observational Method", "PLE", "p-value", "Summary", "p-value"),
       row.names = FALSE,
      caption = "Spearman's Rho - Expert Ratings")
```

Tables 2 and 3 display the correlation between PLE of prevalence or the summary proportions and the Teacher and Expert ratings as well as the estimated p-value for each correlation. This table excludes those 8 observations that were not included in the data we were provided. The correlations between the PLEs and the expert ratings are slighly, although only modestly, worse than the other methods (except for being comprable to the summary in the case of PIR). However, this is does not necessarily point to a disadvantage in the PLEs. When offering a global assessment of any state behavior, that assessment is likely to depend on both the prevalence and the incidence. As discussed previously, a child who is engaged most of the time with few instances of being off task is likely to have a very different learning experience than a child who is engaged a large proportion of the time but also has many instances where they are off task or distracted. Ignoring incidence ignores an important component in state behaviors.

# Incidence

```{r zeta_WIRPIR_CI, echo=FALSE, fig.height = 4}
inc <- ggplot(subset(ple_wide, parm == "zeta"), 
                aes(x = WIRest, y = PIRest, xmin = WIRCI_L, xmax = WIRCI_U,
                    ymin = PIRCI_L, ymax = PIRCI_U
                     ,color = factor(Participant)
                    ))

inc + geom_point() +
          geom_errorbarh()+
          geom_errorbar()+
        labs(title = "PIR and WIR estimates of incidence",
        x= "WIR", y = "PIR") +
        coord_cartesian(xlim = c(0,.75), ylim = c(0,.50))+
        theme_bw()+
        theme(legend.position="none")
```
```{r zeta_WIRPIR, echo = FALSE, fig.height = 4}
inc2 <- ggplot(subset(ple_wide, parm == "zeta"), 
                aes(x = WIRest, y = PIRest, xmin = WIRCI_L, xmax = WIRCI_U,
                    ymin = PIRCI_L, ymax = PIRCI_U
#                     ,color = factor(Participant)
                    ))

inc2 + geom_point() +
#          geom_errorbarh()+
#          geom_errorbar()+
        labs(title = "PIR and WIR estimates of incidence",
        x= "WIR", y = "PIR") +
        geom_abline(slope = 1, intercept = 0)+
        coord_cartesian(xlim = c(0,.75), ylim = c(0,.50))+
        theme_bw()
```
The fifth plot and sixth plots display the PIR and WIR estimates of incidence displayed against one another. The values of incidence have been scaled on a per-interval basis; for instance, if the estimate of incidence is 0.25 we have on average one quarter of a behavior per interval, or about one new behavior every four intervals. The third plot also displays the CIS for incidence. Both plots have been provided because the tight clustering of incidence can make it difficult to interpret the fifth plot.

As with prevalence, the general pattern is that the PIR estimates are lower than the WIR estimates, as well as having some even more extreme deviations than prevalence. Unlike prevalence, we have no direct estimates of incidence to compare our estimates to, so it is difficult to characterize which of the two types of observation procedures best estimate the "true" value based on the data alone.

# Conclusions

Overall, it appears that the WIR estimates for prevalence are better than the PIR estimates for prevalence. The plot of measurement error suggests that the WIR estimates are biased neither systematically upward nor downward. In addition, the magnitude of the bias is much lower, suggesting that our PLE reduces the bias in the WIR estimates considerably. The mean squared error estimate also suggests that the WIR estimates are the better of the two in this case. While the agreement between the expert raters and the WIR estimates of prevalence is not as might be desired, this correlation ignores the important of incidence in characterizing a state behavior like academic engagement.

In addition, the results of our analysis suggests that the PLEs may be sensitive to model mispecification when the PIR model is used with FIR data. Further investigation of the impact of the fractional method on PLE estimates is warranted. 

