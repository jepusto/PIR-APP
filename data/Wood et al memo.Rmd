---
title: "Wood et al Data Memo"
author: "Daniel M. Swan"
date: "Monday, May 11, 2015"
output: pdf_document
---

```{r setup, echo=FALSE, fig.width = 7, fig.height = 7, message = FALSE}
library(ARPobservation)
library(ggplot2)
library(plyr)
library(knitr)
#source("R/PIR-APP.R")
#wood <- read.csv("data/Wood et al data.csv", na.strings = ".", stringsAsFactors = FALSE)
# library(Pusto)
source("../R/PIR-APP.R")
source("../R/parallel setup.R")
wood <- read.csv("Wood et al data.csv", na.strings = ".", stringsAsFactors = FALSE)
```

```{r bootstrap, echo = FALSE, cache = TRUE, warning = FALSE, message = FALSE}
PIR_wrap <- function(data_line, c = 1, d = 0, k_prior = 1, theta_prior = Inf){
  
  U <- data_line[4:70]
  U <- as.numeric(U[(U == 1|U == 0) & !is.na(U)])
  
  results <- PIRbootstrap(U = U, c = c, d = d, coding = data_line[3], 
         penalty_func = Beta_Gamma(k_mu = k_prior, k_lambda = k_prior, 
                                   theta_mu = theta_prior, 
                                   theta_lambda = theta_prior), transform = "exp", iterations = 5000)
  
  summary <- mean(U)
  
  results$summary <- summary
  results
}

source_func <- ls()
invisible(cluster <- start_parallel(source_func))
invisible(clusterEvalQ(cluster, library(ARPobservation)))

ple <- adply(wood, 1, PIR_wrap, k_prior = 1.5, theta_prior = 10, .parallel = TRUE)

stopCluster(cluster)

ple <- ple[,-(4:70)]

cdr <- read.csv("transcribed.csv")[,-1]

ple <- merge(ple, cdr)

ple <- within(ple,{
  CDR <- CDR/100
  MTS <- MTS/100
})

PIR <- subset(ple, Measurement.Type == "PIR")
WIR <- subset(ple, Measurement.Type == "WIR")

names(PIR)[5:9] <- paste0("PIR", names(PIR[5:9]))
names(WIR)[5:9] <- paste0("WIR", names(WIR[5:9]))

ple_wide <- merge(PIR[,-3],WIR[,-3])

save(ple, file = "Wood PLE.RData")
```

This document contains a brief memo on the penalized likelihood estimator (PLE) for PIR and WIR data that we have been developing as applied to a subset of the data from Wood, Hojnoski, Laracy, and Olson (2015). All of the confidence intervals are obtained via a parametric bootstrap procedure.

#Data and Analysis

We were provided with scans of the original hard copies of interval data for 16 of the 24 participants in the study. Two coders transcribed the data to a spreadsheet independently], and then the data was checked for agreement. For the purposes of this analysis, any missing intervals in the middle of the recording were simply discarded and we treated only those intervals containing data as the "complete" record without accounting for missingness in our model.

In order to compare the estimates to a "true" metric, we hand-transcribed the continuous duration recording values of prevalence from Table 1 of Wood et al (2015) -- along with the MTS values, teacher Ratings, and expert ratings -- by matching the summary values of both PIR and WIR to the other estimates in the table. Both the CDR and MTS values are displayed in this document as proportions rather than percentages to conform to the convention we typically use.

#Prevalence and Incidence

When trying to summarize any "state" behavior - that is any behavior with a discrete length rather than one that occurs instantaneously - we are interested in characterizing the behavior in two ways, prevalence and incidence. Prevalence we define as the true proportion of time the behavior occurs. Incidence we define as the rate at which new behaviors occur. Both parameters are necessarily of interest, as a behavior with high prevalence and low incidence is very different from one with high prevalence and high incidence. In the first case, you might have a child who is actively engaged in learning for a large proportion of the time with only a very small number of instances where they are off task. In the second case you have a child who is "engaged" for short bouts, but also has many instances of off-task behavior. 

```{r prep,echo=FALSE}
phi <- ple[ple$parm == "phi",]
names(phi)[4:8] <- paste0("phi", names(phi)[4:8])
zeta <- ple[ple$parm == "zeta",]
names(zeta)[4:8] <- paste0("zeta", names(zeta)[4:8])

parms_wide <- merge(phi[,-4], zeta[,-4])
```

```{r PIRpz, echo = FALSE}
pz <- ggplot(parms_wide, aes(x = phiest,
                             y = zetaest,
                             color = Measurement.Type))

pz + geom_point()+
     labs(title = "Estimates of prevalence and incidence",
          x = "Prevalence",
          y = "Incidence")+
     theme_bw()

# qplot(phiest, zetaest, geom = "point", color = Measurement.Type,
#       main = "Estimates of prevalence and incidence",
#       xlab = "Prevalence",
#       ylab = "Incidence",
#       data = parms_wide) +
#       theme_bw()
```
The first plot displays both PIR and WIR estimates of prevalence plotted against incidence. Here we show that generally WIR estimates have higher incidence and prevalence than the PIR estimates. This may simply be an artifact of the modified PIR used to gather the original data. However, without comparing these values to the true values we can't know if the difference is due to systematic bias, which of the observation methods is biased, and how we might characterize this bias.

#Prevalence

```{r PIR_WIR, echo=FALSE}
prev <- ggplot(subset(ple_wide, parm == "phi"), 
                aes(x = WIRest, y = PIRest))

prev + geom_point() +
#       stat_smooth(method = "lm", se = FALSE) +
        labs(title = "PIR and WIR estimates of prevalence",
        x= "WIR", y = "PIR") +
        coord_cartesian(xlim = c(0,1), ylim = c(0,1))+
        theme_bw()+
        geom_abline(slope = 1, intercept = 0, color = "grey")+
        theme(legend.position="none")

PIRcovered <- with(PIR[PIR$parm == "phi",],sum(PIRCI_L <= CDR & PIRCI_U >= CDR)/16)
WIRcovered <- with(WIR[WIR$parm == "phi",],sum(WIRCI_L <= CDR & WIRCI_U >= CDR)/16)
```

The second plot displays the PLE estimates of prevalence from PIR and WIR plotted against one another. The grey line is a line that passes through $x = y$, where the points would lay if the agreement between the PIR and WIR observations was exact. The plot suggests that the PIR estimates of prevalence are generally lower than the WIR estimates. It is possible that this is attributable to the fact that the PIR used in Wood et al (2015) was a modified version. Putting a minimum on the length of time required for a behavior to "count" as having occurred likely reduced the overall upward bias on prevalence, bias that we attempt to account for in our model.


```{r PIRWIRCDR, echo = FALSE}
qplot(CDR, est, ymin = CI_L, ymax = CI_U, color = Measurement.Type,
      position = position_jitter(w = .05),
      geom = "pointrange",
      main = "Estimates plotted against CDR proportion",
      xlab = "CDR",
      ylab = "Estimates",
      data = subset(ple, parm == "phi"))+
      geom_abline(slope = 1, intercept = 0, color = "grey")+
      coord_cartesian(xlim = c(0.5,1), ylim = c(0,1))+
      theme_bw()
```

The third plot displays the PLE estimates of prevalence from both PIR and WIR plotted against the continuous duration recording proportion. The vertical bars overlaid on the points represents the parametric bootstrapped confidence intervals. We can use this as a benchmark for the bias of our estimates. The grey line is a line through $x = y$, where all of the points would lie if there was perfect agreement between CDR and the other observation methods. This line also allows for easy assessment of whether or not the confidence interval provides coverage of the "true" value. The PIR data appears to be an underestimate of prevalence, while the WIR data is an overestimate, although to a lesser degree. The WIR CIs also generally cover the CDR proportion (coverage = `r round(WIRcovered, digits =2)`) whereas the PIR CIs coverage is slightly lower (coverage = `r round(PIRcovered, digits = 2)`).

```{r PIR_CDR, echo=FALSE}
# qplot(CDR, PIRest, ymin = PIRCI_L, ymax = PIRCI_U, 
#       geom = "pointrange",
#       main = "PIR estimate plotted against CDR proportion",
#       xlab = "CDR",
#       ylab = "PIR",
#       data = subset(ple_wide, parm == "phi"))+
#   theme_bw()
```
```{r WIR_CDR, echo = FALSE}
# qplot(CDR, WIRest, ymin = WIRCI_L, ymax = WIRCI_U, 
#       geom = "pointrange",
#       main = "WIR estimate plotted against CDR proportion",
#       xlab = "CDR",
#       ylab = "PIR",
#       data = subset(ple_wide, parm == "phi"))+
#   theme_bw()
```

```{r meas_error, echo = FALSE}
phi$error <- with(phi, (CDR - phiest)/CDR)
qplot(Participant, error, fill = Measurement.Type, stat = "identity", 
      position = "dodge", geom = "bar", data = phi)+
  scale_x_continuous(breaks = 1:24)+
  theme_bw()+
  theme(legend.position = "top", legend.title = element_blank())
```

The fourth plot displays measurement error relative to the CDR value of prevalence. Unlike the original manuscript, this plot doesn't show the same consistent downward bias for WIR data, although the upward bias for PIR data is still present. The magnitude of the bias for PIR appears slightly larger than in the original manuscript, but the general magnitude of the bias for the WIR estimates is considerably lower.

```{r mse, echo = FALSE}
phi$sq_error <- with(phi, (phiest - CDR)^2)
phi$sq_errorMTS <- with(phi, (MTS - CDR)^2)
phi$sq_errorSum <- with(phi, (summary - CDR)^2)
mean_sq_error <- ddply(phi, .(Measurement.Type), summarize, MSE = sum(sq_error)/16, MSEMTS = sum(sq_errorMTS)/16, MSEsum = sum(sq_errorSum)/16)
kable(mean_sq_error, col.names = c("Measurement Type", "PLE", "MTS", "Summary"), caption = "Error", digits = 4)
```

Table 1 contains the value of the "mean squared error" of the three interval recording procedures. In this case we define "mean squared error" as $\frac{\sum_{i = 1}^{n} (\hat\theta - CDR)^2}{n}$, where $\hat\theta$ is a given estimate of prevalence. The error for PIR PLE is about twice that of the summary measurement, while the error for WIR PLE is very small compared to the summary measurement. The MTS error is roughly equivalant to the WIR error, which is excellent considering that MTS estimates are generally considered to be "unbiased" under very minimal assumptions. The discrepancey between PIR and WIR error is probably an issue of the model not accounting for the slightly different method of PIR used in this data, whereas our model is appropriately specified for the WIR data.

****** `r #here for formatting reasons`

```{r prev_cor, echo = FALSE, fig.align = 'left', warning = FALSE}
Tcors <- ddply(ple[ple$parm == "phi",], .(Measurement.Type), summarize,
               Summary_cor = cor(summary, Teacher, method = "spearman"),
               MTS_cor = cor(MTS, Teacher, method = "spearman"),
               CDR_cor = cor(CDR, Teacher, method = "spearman"),
               Est_cor = cor(est, Teacher, method = "spearman"),
               Summary_p = cor.test(summary, Teacher, method = "spearman")$p.value,
               MTS_p = cor.test(MTS, Teacher, method = "spearman")$p.value,
               CDR_p = cor.test(CDR, Teacher, method = "spearman")$p.value,
               Est_p = cor.test(est, Teacher, method = "spearman")$p.value)

Ecors <- ddply(ple[ple$parm == "phi",], .(Measurement.Type), summarize,
               Summary_cor = cor(summary, Expert.Rater, method = "spearman"),
               MTS_cor = cor(MTS, Expert.Rater, method = "spearman"),
               CDR_cor = cor(CDR, Expert.Rater, method = "spearman"),
               Est_cor = cor(est, Expert.Rater, method = "spearman"),
               Summary_p = cor.test(summary, Expert.Rater, method = "spearman")$p.value,
               MTS_p = cor.test(MTS, Expert.Rater, method = "spearman")$p.value,
               CDR_p = cor.test(CDR, Expert.Rater, method = "spearman")$p.value,
               Est_p = cor.test(est, Expert.Rater, method = "spearman")$p.value)
Tcors <- within(Tcors,{
  Est <- paste0(as.character(round(Est_cor, digits = 2)), ", p = ", as.character(round(Est_p, digits = 2)))
  CDR <- paste0(as.character(round(CDR_cor, digits = 2)), ", p = ", as.character(round(CDR_p, digits = 2)))
  MTS <- paste0(as.character(round(MTS_cor, digits = 2)), ", p = ", as.character(round(MTS_p, digits = 2)))
  summary <- paste0(as.character(round(Summary_cor, digits = 2)), ", p = ", as.character(round(Summary_p, digits = 2)))
})[,-(2:9)]

Ecors <- within(Ecors,{
  Est <- paste0(as.character(round(Est_cor, digits = 2)), ", p = ", as.character(round(Est_p, digits = 2)))
  CDR <- paste0(as.character(round(CDR_cor, digits = 2)), ", p < .01 ")
  MTS <- paste0(as.character(round(MTS_cor, digits = 2)), ", p < .01")
  summary <- paste0(as.character(round(Summary_cor, digits = 2)), ", p = ", as.character(round(Summary_p, digits = 2)))
})[,-(2:9)]
# cors[,2] <- paste0(as.character(round(cors[,2], digits = 2)), ", p > .05")
# cors[,3] <- paste0(as.character(round(cors[,3], digits = 2)), ", p < .05")

kable(Tcors, 
        col.names = c("Observational Method", "Summary Value", "MTS", "CDR", "PLE"), 
      caption = "Spearman's Rho - Teacher Ratings", digits = 2)

kable(Ecors, 
       col.names = c("Observational Method", "Summary Value", "MTS", "CDR", "PLE"), 
      caption = "Spearman's Rho - Expert Ratings", digits = 2)
```

Tables 2 and 3 display the correlation between the different estimates of prevalence and the Teacher and Expert ratings, respectively. This table excludes those 8 observations that were not included in the data we were provided. In the case of the Teachers, none of these correlations are significant. In the case of the Experts, all of them are. The correlations between the PLEs and the expert ratings are slighly, although only modestly, worse than the other methods (except for being comprable to the summary in the case of PIR). However, this is does not necessarily point to a disadvantage in the PLEs. When offering a global assessment of any state behavior, that assessment is likely to depend on both the prevalence and the incidence. A child who is engaged most of the time with few instances of being off task is likely to have a very different learning experience than a child who is engaged a large proportion of the time but also has many instances where they are off task or distracted. Ignoring incidence ignores an important component in state behaviors.

#Incidence

```{r zeta_WIRPIR_CI, echo=FALSE, fig.height = 4}
inc <- ggplot(subset(ple_wide, parm == "zeta"), 
                aes(x = WIRest, y = PIRest, xmin = WIRCI_L, xmax = WIRCI_U,
                    ymin = PIRCI_L, ymax = PIRCI_U
                     ,color = factor(Participant)
                    ))

inc + geom_point() +
          geom_errorbarh()+
          geom_errorbar()+
        labs(title = "PIR and WIR estimates of incidence",
        x= "WIR", y = "PIR") +
        coord_cartesian(xlim = c(0,.75), ylim = c(0,.50))+
        theme_bw()+
        theme(legend.position="none")
```
```{r zeta_WIRPIR, echo = FALSE, fig.height = 4}
inc2 <- ggplot(subset(ple_wide, parm == "zeta"), 
                aes(x = WIRest, y = PIRest, xmin = WIRCI_L, xmax = WIRCI_U,
                    ymin = PIRCI_L, ymax = PIRCI_U
#                     ,color = factor(Participant)
                    ))

inc2 + geom_point() +
#          geom_errorbarh()+
#          geom_errorbar()+
        labs(title = "PIR and WIR estimates of incidence",
        x= "WIR", y = "PIR") +
        geom_abline(slope = 1, intercept = 0)+
        coord_cartesian(xlim = c(0,.75), ylim = c(0,.50))+
        theme_bw()
```
The fifth plot and sixth plots display the PIR and WIR estimates of incidence displayed against one another. The third plot also displays the CIS for incidence. Both plots have been provided because the tight clustering of incidence can make it difficult to interpret the fifth plot. The values of incidence have been scaled on a per-interval basis. That is to say, if the estimate of incidence is 0.25 we have on average one quarter of a behavior per interval, or about one new behavior every four intervals on average. This second value is easily calculated from the first - if we denote incidence as $\zeta$ then the average number of intervals per new behavior is simply $1/\zeta$.

As with prevalence, the general pattern is that the PIR estimates are lower than the WIR estimates, as well as having some even more extreme deviations than prevalence. Unlike prevalence, we have no direct estimates of incidence to compare our estimates to, so it is difficult to characterize which of the two types of observation procedures best estimate the "true" value based on the data alone.

#Conclusions

Generally speaking, it appears that the WIR estimates for prevalence are better than the PIR estimates for prevalence. The plot of measurement error suggests that there these estimates are biased neither systematically upward nor downward. In addition, the magnitude of the bias is much lower, suggesting that our PLE reduces the bias in the WIR estimates considerably. The mean squared error estimate also suggests that the WIR estimates are the better of the two in this case. While the agreement between the expert raters and the WIR estimates of prevalence is not as high as we might like, that correlation ignores the important of incidence in characterizing a state behavior like academic engagement.

